# Funciones de Activaci贸n y sus Derivadas

Este repositorio contiene implementaciones de diferentes funciones de activaci贸n utilizadas en redes neuronales, junto con sus respectivas derivadas y representaciones gr谩ficas.

##  Funciones Implementadas
- **Step Function (Funci贸n Escal贸n)**
- **ReLU (Rectified Linear Unit)**
- **Sigmoid**
- **Tanh (Tangente Hiperb贸lica)**

##  Estructura del Repositorio
```
activaciones/
 step.py          # Implementaci贸n de la funci贸n escal贸n
 relu.py          # Implementaci贸n de la funci贸n ReLU
 sigmoid.py       # Implementaci贸n de la funci贸n Sigmoide
 tanh.py          # Implementaci贸n de la funci贸n Tangente Hiperb贸lica
 utils.py         # Funciones auxiliares
 main.py          # Script principal para visualizar las funciones
 README.md        # Documentaci贸n del proyecto
```

##  Instalaci贸n y Uso
### 1锔 Clonar el repositorio
```bash
git clone https://github.com/tu_usuario/activaciones.git
cd activaciones
```
### 2锔 Instalar dependencias
Se recomienda usar un entorno virtual:
```bash
pip install -r requirements.txt
```

### 3锔 Ejecutar el script
Para visualizar las funciones de activaci贸n y sus derivadas:
```bash
python main.py
```

##  Ejemplo de Gr谩ficas
Al ejecutar el script, se mostrar谩n las gr谩ficas de cada funci贸n de activaci贸n junto con su derivada.

![Ejemplo de Gr谩fica](ruta_a_imagen_ejemplo.png)

##  Tecnolog铆as Usadas
- Python 3.x
- Matplotlib
- NumPy

##  Contribuci贸n
Si deseas mejorar el proyecto, 隆toda contribuci贸n es bienvenida! Puedes hacer un fork y enviar un pull request.

##  Licencia
Este proyecto est谩 bajo la licencia MIT. Puedes usarlo libremente para fines educativos y de desarrollo.
