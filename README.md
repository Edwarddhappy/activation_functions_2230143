# Funciones de Activación y sus Derivadas

Este repositorio contiene implementaciones de diferentes funciones de activación utilizadas en redes neuronales, junto con sus respectivas derivadas y representaciones gráficas.

## 📌 Funciones Implementadas
- **Step Function (Función Escalón)**
- **ReLU (Rectified Linear Unit)**
- **Sigmoid**
- **Tanh (Tangente Hiperbólica)**

## 📂 Estructura del Repositorio
```
activaciones/
│── step.py          # Implementación de la función escalón
│── relu.py          # Implementación de la función ReLU
│── sigmoid.py       # Implementación de la función Sigmoide
│── tanh.py          # Implementación de la función Tangente Hiperbólica
│── utils.py         # Funciones auxiliares
│── main.py          # Script principal para visualizar las funciones
│── README.md        # Documentación del proyecto
```

## 🚀 Instalación y Uso
### 1️⃣ Clonar el repositorio
```bash
git clone https://github.com/tu_usuario/activaciones.git
cd activaciones
```
### 2️⃣ Instalar dependencias
Se recomienda usar un entorno virtual:
```bash
pip install -r requirements.txt
```

### 3️⃣ Ejecutar el script
Para visualizar las funciones de activación y sus derivadas:
```bash
python main.py
```

## 📊 Ejemplo de Gráficas
Al ejecutar el script, se mostrarán las gráficas de cada función de activación junto con su derivada.

![Ejemplo de Gráfica](ruta_a_imagen_ejemplo.png)

## 🛠 Tecnologías Usadas
- Python 3.x
- Matplotlib
- NumPy

## 📌 Contribución
Si deseas mejorar el proyecto, ¡toda contribución es bienvenida! Puedes hacer un fork y enviar un pull request.

## 📄 Licencia
Este proyecto está bajo la licencia MIT. Puedes usarlo libremente para fines educativos y de desarrollo.
